%matplotlib inline

import numpy as np
from matplotlib import pyplot as plt
import time
import os
import torch
import torch.nn as nn
from torch.autograd import Variable
from torch.utils.data import Dataset, DataLoader
import setproctitle
from tqdm import tqdm
from torch.utils import data
os.environ["CUDA_DEVICE_ORDER"]="PCI_BUS_ID"   # see issue #152
os.environ["CUDA_VISIBLE_DEVICES"]="1"
cuda = torch.cuda.is_available()
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
datasets = np.load('c.npy', allow_pickle=True)
dataset = np.loadtxt('real.data')
for i in range(len(dataset)):
    dataset[i] = torch.tensor(dataset[i]).long()
for i in range(len(datasets)):
    datasets[i] = torch.tensor(datasets[i]).long()
class LanguageModelDataLoader(Dataset):
  def __init__(self, X):
    self.X = X
  def __len__(self):
    return len(self.X)
  def __getitem__(self, index):
    # Extract data and label
    data = self.X[index][:-2]
    label = self.X[index][1:]
    label = label[:-1]
    
    return torch.tensor(data).float(), torch.tensor(label).long()
# model

class LanguageModel(nn.Module):
    """
        TODO: Define your model here
    """
    def __init__(self, vocab_size):
        super(LanguageModel, self).__init__()
        self.encoder = nn.Embedding(vocab_size, 512)
        self.rnn = nn.LSTM(         
            input_size=512,
            hidden_size=512,        
            num_layers=3,           
            batch_first=True)
        self.decoder = nn.Linear(512, vocab_size)
        self.decoder.weight = self.encoder.weight
    def init_weights(self):
        initrange = 0.1
        self.encoder.weight.data.uniform_(-initrange, initrange)
        self.decoder.bias.data.fill_(0)
    def forward(self, x):
        x = torch.tensor(x).long()
        x = x.to(device)
        z = self.encoder(x)
        out = self.rnn(z)[0]
        out = self.decoder((out))
        return out
    
class LanguageModelTrainer:
    def __init__(self, model, loader, max_epochs=1, run_id='exp'):
        """
            Use this class to train your model
        """
        # feel free to add any other parameters here
        self.model = model
        self.loader = loader
        self.train_losses = []
        self.val_losses = []
        self.predictions = []
        self.predictions_test = []
        self.generated_logits = []
        self.generated = []
        self.generated_logits_test = []
        self.generated_test = []
        self.epochs = 0
        self.max_epochs = max_epochs
        self.run_id = run_id
        
        # TODO: Define your optimizer and criterion here
        self.optimizer = torch.optim.Adam(model.parameters(), lr= 1e-3)
        self.criterion = torch.nn.CrossEntropyLoss(reduction='none')

    def train(self):
        self.model.train() # set to training mode
        epoch_loss = 0
        num_batches = 0
        for batch_num, (inputs, targets) in enumerate(self.loader):
            epoch_loss += self.train_batch(inputs, targets)
        epoch_loss = epoch_loss / (batch_num + 1)
        self.epochs += 1
        print('[TRAIN]  Epoch [%d/%d]   Loss: %.4f'
                      % (self.epochs + 1, self.max_epochs, epoch_loss))
        self.train_losses.append(epoch_loss)

    def train_batch(self, inputs, targets):
        """ 
            TODO: Define code for training a single batch of inputs
        
        """
        inputs = inputs.to(device)
        targets = targets.to(device)
        running_loss = 0
        predict = model(inputs)
        loss_value = self.criterion(torch.transpose(predict,1,2), targets)
        loss_value = torch.sum(loss_value)/int(len(targets)*len(targets[0]))        
        running_loss += loss_value.item()          
        loss_value.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 2)
        self.optimizer.step()    
        return running_loss
    def test(self):
        # don't change these        
        self.model.eval() # set to eval mode
        epoch_loss = 0
        num_batches = 0
        epoch_loss = self.test_batch(torch.tensor(dataset[:,0:46]).float(), torch.tensor(dataset[:,1:47]).long())
        print(epoch_loss)

    def test_batch(self, inputs, targets):
        """ 
            TODO: Define code for training a single batch of inputs
        
        """
        inputs = inputs.to(device)
        targets = targets.to(device)
        running_loss = 0
        predict = model(inputs)
        predict = torch.max(predict,2)[1]
        for i in range(len(predict)):
            for j in range(len(predict[i])):
                if predict[i][j] == targets[i][j]:
                    running_loss += 1
        running_loss = running_loss/int(len(targets)*len(targets[0]))               
        return running_loss

    def save(self):
        # don't change these
        model_path = os.path.join('experiments', self.run_id, 'model-{}.pkl'.format(self.epochs))
        torch.save({'state_dict': self.model.state_dict()},model_path)
NUM_EPOCHS =100
BATCH_SIZE = 100
run_id = str(int(time.time()))
if not os.path.exists('./experiments'):
    os.mkdir('./experiments')
os.mkdir('./experiments/%s' % run_id)
print("Saving models, predictions, and generated words to ./experiments/%s" % run_id)
model = LanguageModel(8606).to(device)

train_dataset = LanguageModelDataLoader(datasets)
train_loader = data.DataLoader(
           dataset=train_dataset, 
           batch_size=100,
           num_workers=48,
           shuffle=True,
           drop_last=False)
trainer = LanguageModelTrainer(model=model, loader=train_loader,max_epochs=NUM_EPOCHS, run_id=run_id)
best_nll = 1e30 
for epoch in range(NUM_EPOCHS):
    trainer.train()
    if epoch >90:
        trainer.test()
        trainer.save()
